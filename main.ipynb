{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713e0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone as Pine\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from gemini_client import Gemini_client as Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296fab6",
   "metadata": {},
   "source": [
    "# List of Prompts to send for the Experiments\n",
    "\n",
    "We are using 3 for variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3d9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"Based on the provided customer feedback, generate a single-paragraph summary of the overall sentiment \n",
    "    regarding the iPhone 16. Identify the single most frequently mentioned positive attribute and the \n",
    "    single most frequent complaint mentioned about the device in the data. Be sure both points are directly supported \n",
    "    by the feedback.\"\"\",\n",
    "    \n",
    "    \"\"\"Generate a report structured as a pro-and-con list specifically comparing the iPhone 16 vs. the Samsung S24. \n",
    "    The report must include three distinct product areas (e.g., Battery Life, Design, Performance) where customers \n",
    "    clearly prefer the iPhone and three distinct product areas where customers clearly prefer the Samsung. You must \n",
    "    source all six points entirely from the feedback.\"\"\",\n",
    "    \n",
    "    \"\"\"Using the provided customer feedback, identify which of the two devices — iPhone 16 or Samsung S24 — is currently \n",
    "    perceived as weaker in overall satisfaction. Then, propose a **single, evidence-backed product or experience change** \n",
    "    that would most directly address the top recurring customer pain point for that device. Your proposal must cite \n",
    "    specific feedback patterns or quotes from the data to justify the recommendation.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f721965",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = Gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22582e3e",
   "metadata": {},
   "source": [
    "# Experiment 1: Generating output on pre-trained LLM's without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        response = gemini.generate_content(prompt)\n",
    "    \n",
    "        data.append({\n",
    "            \"Prompt\": prompt,\n",
    "            \"Response\": response\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Exception found: {e}\")\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "with pd.ExcelWriter(r\"Prompt_responses.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=\"Raw_run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add9f53",
   "metadata": {},
   "source": [
    "# Required Variables for Experiments 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b7a622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_excel(r\"labeled_comments.xls\")\n",
    "\n",
    "len(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd504349",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = {}\n",
    "batch_size = 20\n",
    "\n",
    "for i, row in comments_df.iterrows():\n",
    "    if type(row[\"comment_text\"]) != str:\n",
    "        row[\"comment_text\"] = str(row[\"comment_text\"])\n",
    "    \n",
    "    batch_id = (i // batch_size) + 1\n",
    "    key = f\"batch_{batch_id}\"\n",
    "    batches.setdefault(key, \"\")\n",
    "    batches[key] += row[\"comment_text\"] + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d78d677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7dcf8",
   "metadata": {},
   "source": [
    "# Experiment 2: Augmentation of Data in Raw format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad10f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from batch index 1\n",
      "✅ Completed batch_2\n",
      "✅ Completed batch_3\n",
      "✅ Completed batch_4\n",
      "✅ Completed batch_5\n",
      "✅ Completed batch_6\n",
      "⚠️ Model overloaded at batch_7. Retrying in 5s...\n",
      "⚠️ Model overloaded at batch_7. Retrying in 10s...\n",
      "⚠️ Model overloaded at batch_7. Retrying in 20s...\n",
      "✅ Completed batch_7\n",
      "✅ Completed batch_8\n",
      "✅ Completed batch_9\n",
      "✅ Completed batch_10\n",
      "✅ Completed batch_11\n",
      "⚠️ Model overloaded at batch_12. Retrying in 5s...\n",
      "⚠️ Model overloaded at batch_12. Retrying in 10s...\n",
      "✅ Completed batch_12\n",
      "✅ Completed batch_13\n",
      "✅ Completed batch_14\n",
      "✅ Completed batch_15\n",
      "⚠️ Model overloaded at batch_16. Retrying in 5s...\n",
      "⚠️ Model overloaded at batch_16. Retrying in 10s...\n",
      "✅ Completed batch_16\n",
      "✅ Completed batch_17\n",
      "⚠️ Model overloaded at batch_18. Retrying in 5s...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "batches_list = list(batches.items())\n",
    "retry_delay = 5          # initial sleep (seconds)\n",
    "max_delay = 300          # max backoff (5 min)\n",
    "checkpoint_file = \"checkpoint.json\"\n",
    "\n",
    "# --- Load last checkpoint if exists\n",
    "try:\n",
    "    with open(checkpoint_file, \"r\") as f:\n",
    "        start_index = json.load(f).get(\"last_index\", 0)\n",
    "except FileNotFoundError:\n",
    "    start_index = 0\n",
    "\n",
    "print(f\"Resuming from batch index {start_index}\")\n",
    "\n",
    "for idx in range(start_index, len(batches_list)):\n",
    "    batch_name, prompt = batches_list[idx]\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = gemini.responses_experiment_2(prompt)\n",
    "\n",
    "            print(f\"✅ Completed {batch_name}\")\n",
    "\n",
    "            # Reset delay after success\n",
    "            retry_delay = 5\n",
    "\n",
    "            # Save checkpoint\n",
    "            with open(checkpoint_file, \"w\") as f:\n",
    "                json.dump({\"last_index\": idx + 1}, f)\n",
    "\n",
    "            # Respect pacing\n",
    "            time.sleep(5)\n",
    "            break  # proceed to next batch\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"503\" in str(e):\n",
    "                print(f\"⚠️ Model overloaded at {batch_name}. Retrying in {retry_delay}s...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay = min(retry_delay * 2, max_delay)  # exponential backoff\n",
    "            else:\n",
    "                print(f\"❌ Unexpected error at {batch_name}: {e}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b73606",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    payload = {}\n",
    "    response = gemini.responses_experiment_2(prompt)\n",
    "    payload[prompt] = response\n",
    "    data.append(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61451042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "with pd.ExcelWriter(r\"Prompt_responses.xlsx\", engine=\"openpyxl\", mode='a') as writer:\n",
    "    df.to_excel(writer, \"Raw_Message_Augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662916f",
   "metadata": {},
   "source": [
    "# Experiment 3: RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881e5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pine_client import PineConeDB\n",
    "\n",
    "pine = PineConeDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605dc213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n",
      "Uploading records......\n",
      "Uploaded\n"
     ]
    }
   ],
   "source": [
    "for batch_name, batch_text in batches.items():\n",
    "    record = [{\n",
    "        \"id\": batch_name,\n",
    "        \"chunk_text\": batch_text   # field Pinecone will embed automatically\n",
    "    }]\n",
    "    pine.upsert_data(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba35e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error for prompt: Using the provided customer feedback, identify which of the ... -> 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # --- Retrieve context from Pinecone\n",
    "    result = pine.search_index(prompt)\n",
    "\n",
    "    # defensive parsing to avoid key errors\n",
    "    hits = result.get(\"result\", {}).get(\"hits\", [])\n",
    "\n",
    "    relevant_texts = []\n",
    "    for hit in hits:\n",
    "        # handle any malformed hit objects\n",
    "        chunk_text = hit.get(\"fields\", {}).get(\"chunk_text\", None)\n",
    "        if chunk_text:\n",
    "            relevant_texts.append(chunk_text.strip())\n",
    "\n",
    "    # join top-k results into a context block\n",
    "    final_text = \"\\n\\n\".join(relevant_texts)\n",
    "    if not final_text:\n",
    "        final_text = \"[No relevant context retrieved.]\"\n",
    "\n",
    "    # --- Construct context-aware prompt\n",
    "    final_prompt = (\n",
    "        f\"Context retrieved from indexed customer feedback:\\n{final_text}\\n\\n\"\n",
    "        f\"Task:\\n{prompt}\"\n",
    "    )\n",
    "\n",
    "    # --- Generate model response\n",
    "    try:\n",
    "        response = gemini.generate_content(final_prompt)\n",
    "        # extract plain text response if wrapped\n",
    "        response_text = (\n",
    "            response.text\n",
    "            if hasattr(response, \"text\")\n",
    "            else str(response)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error for prompt: {prompt[:60]}... -> {e}\")\n",
    "        response_text = f\"ERROR: {e}\"\n",
    "\n",
    "    # --- Store result\n",
    "    data.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"Retrieved_Context\": final_text,\n",
    "        \"Response\": response_text\n",
    "    })\n",
    "    \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6894f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# save to excel\n",
    "with pd.ExcelWriter(r\"Prompt_responses.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=\"rag_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c682f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last prompt failed\n",
    "prompt = prompts[2]\n",
    "\n",
    "result = pine.search_index(prompt)\n",
    "\n",
    "# defensive parsing to avoid key errors\n",
    "hits = result.get(\"result\", {}).get(\"hits\", [])\n",
    "relevant_texts = []\n",
    "\n",
    "for hit in hits:\n",
    "    # handle any malformed hit objects\n",
    "    chunk_text = hit.get(\"fields\", {}).get(\"chunk_text\", None)\n",
    "    if chunk_text:\n",
    "        relevant_texts.append(chunk_text.strip())\n",
    "\n",
    "# join top-k results into a context block\n",
    "final_text = \"\\n\\n\".join(relevant_texts)\n",
    "\n",
    "if not final_text:\n",
    "    final_text = \"[No relevant context retrieved.]\"\n",
    "# --- Construct context-aware prompt\n",
    "final_prompt = (\n",
    "    f\"Context retrieved from indexed customer feedback:\\n{final_text}\\n\\n\"\n",
    "    f\"Task:\\n{prompt}\"\n",
    ")\n",
    "\n",
    "# --- Generate model response\n",
    "try:\n",
    "    response = gemini.generate_content(final_prompt)\n",
    "    # extract plain text response if wrapped\n",
    "    response_text = (\n",
    "        response.text\n",
    "        if hasattr(response, \"text\")\n",
    "        else str(response)\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error for prompt: {prompt[:60]}... -> {e}\")\n",
    "    response_text = f\"ERROR: {e}\"\n",
    "    \n",
    "# --- Store result\n",
    "data.append({\n",
    "    \"Prompt\": prompt,\n",
    "    \"Retrieved_Context\": final_text,\n",
    "    \"Response\": response_text\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "432af12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided customer feedback, the **Samsung S24** is perceived as weaker in overall satisfaction due to recurring issues with performance under real-world stress.\\n\\nWhile some users complain about the iPhone\\'s restrictive ecosystem or specific model flaws (e.g., iPhone 14 Pro battery), the feedback against Samsung points to a more fundamental failure in core functionality: reliability and sustained performance.\\n\\n---\\n\\n### **Proposed Change for Samsung:**\\n\\n**Implement an advanced, next-generation vapor chamber cooling system focused on sustained performance during intensive, multi-application use.**\\n\\nThis hardware enhancement directly addresses the top recurring customer pain point for Samsung devices: thermal throttling, overheating, and significant battery drain during real-world \"workhorse\" scenarios.\\n\\n#### **Evidence and Justification:**\\n\\nThe feedback indicates a clear pattern where users find Samsung phones impressive in features but lacking in reliability when pushed. A user who switched from an iPhone 13 Pro Max to a Samsung S25 Ultra (reflecting on the current generation\\'s experience) provides the most direct evidence:\\n\\n*   **The Core Problem:** The user contrasts the iPhone as a \"boring but a real workhorse\" against their Samsung experience, stating, **\"when the going gets tough the samsung overheats and drains battery.\"**\\n\\n*   **Real-World Failure:** This isn\\'t a benchmark test; it\\'s a failure during daily intensive use. The user specifies that while **\"traveling about and doing multiple things... constantly using various apps such as banking apps and uber and Google maps and camera and messaging etc... the samsung just can\\'t handle it.\"**\\n\\n*   **Pattern of Issues:** This experience is corroborated by other feedback. One user with an S23 Ultra reports it **\"gets hot as hell if I turn on anything that uses some cpu,\"** indicating this is a persistent issue across recent flagship generations. Another user simply states, **\"I have s24u and it laggas.\"**\\n\\nAn improved thermal management system is the most direct solution. It would allow the processor to maintain peak performance for longer periods without overheating. This would prevent the system from throttling (causing lag) and reduce the excessive heat that leads to rapid battery drain, directly solving the \"can\\'t handle it\" complaint and closing the perceived reliability gap with the iPhone\\'s \"workhorse\" reputation.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
